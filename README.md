# GPT-2 - Modelo de Linguagem com auto-atenção e máscaras causais

Esse projeto foi feito para a disciplina de mestrado da Unicamp IA024 - Redes Neurais Profundas para Processamento de Linguagem Natural.

Neste projeto, foi treinado um modelo de linguagem com múltiplas cabeças de auto-atenção e com máscara causal para prever a próxima palavra de um texto, dada as palavras anteriores como entrada.. A máscara causal é necessária para que o modelo não tenha acesso a palavras futuras, que é a abordagem usada por grandes modelos de linguagem, como o GPT.

A base de dados utilizada foram textos retirados de livros do Machado de Assis (https://github.com/ethelbeluzzi/projetomachado)
